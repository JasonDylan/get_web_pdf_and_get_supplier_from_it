{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import timeout_decorator\n",
    "import time\n",
    "import globals\n",
    "import re\n",
    "import ast\n",
    "import traceback\n",
    "COMPLETION_MODEL = globals.COMPLETION_MODEL\n",
    "API_KEY = globals.API_KEY\n",
    "\n",
    "\n",
    "# @timeout_decorator.timeout(100)\n",
    "def chat_gpt_turbo(message_our,COMPLETION_MODEL,n=1,max_tokens=3000):\n",
    "\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=COMPLETION_MODEL,\n",
    "        max_tokens = max_tokens,\n",
    "        n = n,\n",
    "        temperature = 0,\n",
    "        messages=message_our\n",
    "        )\n",
    "\n",
    "    return completion.choices[0].message\n",
    "\n",
    "\n",
    "def entity_extraction_func(text):\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        As an Operations Management Researcher and Natural Language Processing (NLP) Engineer, your primary task is to extract key information about suppliers from the provided text. Please follow these steps strictly:\n",
    "        1 Entity Recognition: Identify and extract all supplier names present in the text. These are the names of companies or organizations mentioned as distinct entities in the text.\n",
    "        2 Text Summary: Summarize the main activities or characteristics of each identified supplier based on the content of the text. Ensure that the summary accurately reflects the information in the text.\n",
    "        Adhere to this format:\n",
    "        {\n",
    "            'Supplier Company Name': 'Summary of content about current Supplier',\n",
    "            ...\n",
    "        }.\n",
    "        Do not add or change any content; simply fill in the relevant information.\n",
    "\n",
    "        Example input text:\n",
    "        \"...Solar Solutions recently completed a large-scale solar energy project, which successfully increased the region's renewable energy supply...\"\n",
    "        Example output:\n",
    "        { 'Solar Solutions': 'Completed a large-scale solar energy project, increasing the region's renewable energy supply'. }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"Here is the text: \\n {text}\" \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    out_put = response.choices[0].message.content\n",
    "    # print(out_put)\n",
    "\n",
    "    try:\n",
    "\n",
    "        # 使用正则表达式匹配可能的字典格式\n",
    "        dict_match = re.search(r'\\{.*?\\}', out_put, re.DOTALL)\n",
    "        # # 将匹配的文本中的单引号转换为双引号，并清理内部的双引号\n",
    "        # dict_str = dict_match.group().replace(\"'\", '\"')\n",
    "        # dict_str = re.sub(r'(?<!\\\\)\"', '', dict_str)  # 移除非转义的双引号\n",
    "\n",
    "        # 将匹配的文本处理为符合字典格式的字符串\n",
    "        dict_str = dict_match.group()\n",
    "        dict_str = re.sub(r\"[^a-zA-Z0-9{}:,. '\\\"\\[\\]]\", '', dict_str)  # 移除不合法的字符\n",
    "        dict_str = re.sub(r'(?<!\")\\b\\w+\\b(?=\":)', lambda x: '\"' + x.group() + '\"', dict_str)  # 确保键被双引号包围\n",
    "        # 尝试解析字典\n",
    "        data = ast.literal_eval(dict_str)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {}\n",
    "        # 可以在此处添加其他错误处理逻辑，如返回默认值或继续执行其他任务\n",
    "\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_data(text, entity_extraction_func):\n",
    "    try:\n",
    "        # 使用正则表达式匹配可能的字典格式\n",
    "        dict_match = re.search(r'\\{.*?\\}', text, re.DOTALL)\n",
    "        if dict_match:\n",
    "            # 将匹配的文本中的单引号转换为双引号，并清理内部的双引号\n",
    "            # dict_str = dict_match.group().replace(\"'\", '\"')\n",
    "            # dict_str = re.sub(r'(?<!\\\\)\"', '', dict_str)  # 移除非转义的双引号\n",
    "\n",
    "            # 将匹配的文本处理为符合字典格式的字符串\n",
    "            dict_str = dict_match.group()\n",
    "            dict_str = re.sub(r\"[^a-zA-Z0-9{}:,. '\\\"\\[\\]]\", '', dict_str)  # 移除不合法的字符\n",
    "            dict_str = re.sub(r'(?<!\")\\b\\w+\\b(?=\":)', lambda x: '\"' + x.group() + '\"', dict_str)  # 确保键被双引号包围\n",
    "\n",
    "            try:\n",
    "                # 尝试解析字典\n",
    "                data = ast.literal_eval(dict_str)\n",
    "            except (SyntaxError, ValueError):\n",
    "                # 如果解析失败，使用命名实体识别函数处理文本\n",
    "                data = entity_extraction_func(text)\n",
    "        else:\n",
    "            # 如果没有匹配到字典格式，同样使用命名实体识别函数\n",
    "            data = entity_extraction_func(text)\n",
    "\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return None \n",
    "\n",
    "def save_data(data, path, company_name, ):\n",
    "    \n",
    "        # 创建 DataFrame\n",
    "        df = pd.DataFrame(list(data.items()), columns=['Supplier', 'Content of Supplier'])\n",
    "\n",
    "        # # 添加编号列\n",
    "        # df.insert(0, 'Number', range(1, len(df) + 1))\n",
    "\n",
    "        # # 构建文件路径\n",
    "        # file_path = f'{path}/{company_name}_suppliers.csv'\n",
    "        # 构建文件路径\n",
    "        file_path = f'{path}/{company_name}.csv'\n",
    "\n",
    "        # 检查路径是否存在，如果不存在，则创建它\n",
    "        directory = os.path.dirname(file_path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        # 检查文件是否存在，根据情况选择写入模式\n",
    "        if os.path.exists(file_path):\n",
    "            # 文件存在，使用追加模式\n",
    "            df.to_csv(file_path, mode='a', header=False, index=False, sep=\"\\t\")\n",
    "        else:\n",
    "            # 文件不存在，使用写入模式\n",
    "            df.to_csv(file_path, index=False, sep=\"\\t\")\n",
    "\n",
    "def read_and_replace_newlines(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # 读取文件内容\n",
    "        text = file.read()\n",
    "\n",
    "    # 删除额外的空白字符和无用符号\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # 替换多个连续换行符为单个换行符\n",
    "    text = re.sub(r'\\s+\\n', '\\n', text)  # 删除行末的空白字符\n",
    "    text = re.sub(r'\\n\\s+', '\\n', text)  # 删除行首的空白字符\n",
    "    text = re.sub(r'[^A-Za-z0-9.,;:\\'\"\\(\\)\\[\\]\\n]+', ' ', text)  # 删除除基本标点符号和数字字母外的所有字符\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_text_into_parts(text_org, num_parts=10):\n",
    "    extracted_text = text_org\n",
    "    # 计算每部分的大致长度\n",
    "    part_length = len(extracted_text) // num_parts\n",
    "\n",
    "    # 初始化结果列表和当前部分的开始索引\n",
    "    parts = []\n",
    "    start = 0\n",
    "\n",
    "    for _ in range(num_parts - 1):\n",
    "        # 计算这部分应当结束的大致位置\n",
    "        end = start + part_length\n",
    "\n",
    "        # 如果不是在文本末尾，尽量在句子或单词结束处分割\n",
    "        if end < len(extracted_text):\n",
    "            while end < len(extracted_text) and extracted_text[end] not in \" .,;?!\":\n",
    "                end += 1\n",
    "\n",
    "        # 截取当前部分\n",
    "        parts.append(extracted_text[start:end])\n",
    "\n",
    "        # 更新下一部分的开始位置\n",
    "        start = end\n",
    "\n",
    "    # 添加最后一部分\n",
    "    parts.append(extracted_text[start:])\n",
    "\n",
    "    return parts\n",
    "\n",
    "def extract_10k_from_text(text_org):\n",
    "    \n",
    "    # 10k 特有的截取过程\n",
    "    pattern = r\"Item\\s1\\..*Item\\s1B\\.\"\n",
    "    result = re.search(pattern, text_org, re.DOTALL)\n",
    "    if result:\n",
    "        extracted_text = result.group(0)\n",
    "        len_extracted_text = len(extracted_text)\n",
    "        print(f\"{len_extracted_text=} {extracted_text}\")\n",
    "    else:\n",
    "        extracted_text = None\n",
    "        print(\"No match found.\")\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共待处理文档74个\n",
      "文档列表如下：\n",
      "txt_path_list= ['AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2013_12_31_aagaa10k-20131231.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2014_12_31_d829913d10k.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2015_12_31_d78287d10k.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2016_12_31_d286458d10k.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2017_12_31_a10k123117.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2018_12_31_a10k123118.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2019_12_31_a10k123119.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2020_12_31_aal-20201231.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2021_12_31_aal-20211231.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2022_12_31_aal-20221231.txt', 'AAPL_320193-AAPL_APPLE_INC_2013_09_28_d590790d10k.txt', 'AAPL_320193-AAPL_APPLE_INC_2014_09_27_d783162d10k.txt', 'AAPL_320193-AAPL_APPLE_INC_2015_09_26_d17062d10k.txt', 'AAPL_320193-AAPL_APPLE_INC_2016_09_24_a201610-k9242016.txt', 'AAPL_320193-AAPL_APPLE_INC_2017_09_30_a10-k20179302017.txt', 'AAPL_320193-AAPL_APPLE_INC_2018_09_29_a10-k20189292018.txt', 'AAPL_320193-AAPL_APPLE_INC_2019_09_28_a10-k20199282019.txt', 'AAPL_320193-AAPL_APPLE_INC_2020_09_26_aapl-20200926.txt', 'AAPL_320193-AAPL_APPLE_INC_2021_09_25_aapl-20210925.txt', 'AAPL_320193-AAPL_APPLE_INC_2022_09_24_aapl-20220924.txt', 'AAPL_320193-AAPL_APPLE_INC_2023_09_30_aapl-20230930.txt', 'ABBV_1551152-ABBV_ABBVIE_INC_2014_12_31_a2223058z10-k.txt', 'ABBV_1551152-ABBV_ABBVIE_INC_2015_12_31_a2227341z10-k.txt', 'ABBV_1551152-ABBV_ABBVIE_INC_2016_12_31_abbv-12312016x10k.txt', 'ABBV_1551152-ABBV_ABBVIE_INC_2017_12_31_abbv-20171231x10k.txt', 'ABBV_1551152-ABBV_ABBVIE_INC_2018_12_31_abbv-20181231x10k.txt', 'ABBV_1551152-ABBV_ABBVIE_INC_2019_12_31_abbv-20191231x10k.txt', 'ABBV_1551152-ABBV_ABBVIE_INC_2020_12_31_abbv-20201231.txt', 'ABBV_1551152-ABBV_ABBVIE_INC_2021_12_31_abbv-20211231.txt', 'ABBV_1551152-ABBV_ABBVIE_INC_2022_12_31_abbv-20221231.txt', 'ABNB_1559720-ABNB_AIRBNB_INC_2020_12_31_airbnb-10k.txt', 'ABNB_1559720-ABNB_AIRBNB_INC_2021_12_31_abnb-20211231.txt', 'ABNB_1559720-ABNB_AIRBNB_INC_2022_12_31_abnb-20221231.txt', 'ABT_1800-ABT_ABBOTT_LABORATORIES_2016_12_31_a2230875z10-k.txt', 'ABT_1800-ABT_ABBOTT_LABORATORIES_2017_12_31_a2234264z10-k.txt', 'ABT_1800-ABT_ABBOTT_LABORATORIES_2018_12_31_a2237733z10-k.txt', 'ABT_1800-ABT_ABBOTT_LABORATORIES_2019_12_31_abt-20191231x10k59d41b.txt', 'ABT_1800-ABT_ABBOTT_LABORATORIES_2020_12_31_abt-20201231x10k.txt', 'ABT_1800-ABT_ABBOTT_LABORATORIES_2021_12_31_abt-20211231x10k.txt', 'ABT_1800-ABT_ABBOTT_LABORATORIES_2022_12_31_abt-20221231.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2011_12_31_a2207402z10-k.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2012_12_31_a2213076z10-k.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2013_12_31_a201310-k.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2014_12_31_a201410-k.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2015_12_31_a201510-k.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2016_12_31_a201610-k.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2017_12_31_a201710-k.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2018_12_31_a201810-k.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2019_12_31_a201910-k.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2020_12_31_acgl-20201231.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2021_12_31_acgl-20211231.txt', 'ACGL_947484-ACGL_ARCH_CAPITAL_GROUP_LTD_2022_12_31_acgl-20221231.txt', 'ACN_1467373-ACN_ACCENTURE_PLC_2020_08_31_acn-20200831.txt', 'ACN_1467373-ACN_ACCENTURE_PLC_2021_08_31_acn-20210831.txt', 'ACN_1467373-ACN_ACCENTURE_PLC_2022_08_31_acn-20220831.txt', 'ACN_1467373-ACN_ACCENTURE_PLC_2023_08_31_acn-20230831.txt', 'ADBE_796343-ADBE_ADOBE_INC_2015_11_27_adbe10kfy15.txt', 'ADBE_796343-ADBE_ADOBE_INC_2016_12_02_adbe10kfy16.txt', 'ADBE_796343-ADBE_ADOBE_INC_2017_12_01_adbe10kfy17.txt', 'ADBE_796343-ADBE_ADOBE_INC_2018_11_30_adbe10kfy18.txt', 'ADBE_796343-ADBE_ADOBE_INC_2019_11_29_adbe10kfy19.txt', 'ADBE_796343-ADBE_ADOBE_INC_2020_11_27_adbe-20201127.txt', 'ADBE_796343-ADBE_ADOBE_INC_2021_12_03_adbe-20211203.txt', 'ADBE_796343-ADBE_ADOBE_INC_2022_12_02_adbe-20221202.txt', 'A_1090872-A_AGILENT_TECHNOLOGIES_INC_2013_10_31_a-10312013x10k.txt', 'A_1090872-A_AGILENT_TECHNOLOGIES_INC_2014_10_31_a-10312014x10k.txt', 'A_1090872-A_AGILENT_TECHNOLOGIES_INC_2015_10_31_a-10312015x10k.txt', 'A_1090872-A_AGILENT_TECHNOLOGIES_INC_2016_10_31_a-10312016x10k.txt', 'A_1090872-A_AGILENT_TECHNOLOGIES_INC_2017_10_31_a-10312017x10k.txt', 'A_1090872-A_AGILENT_TECHNOLOGIES_INC_2018_10_31_a-10312018x10k.txt', 'A_1090872-A_AGILENT_TECHNOLOGIES_INC_2019_10_31_a-10312019x10k.txt', 'A_1090872-A_AGILENT_TECHNOLOGIES_INC_2020_10_31_a-20201031.txt', 'A_1090872-A_AGILENT_TECHNOLOGIES_INC_2021_10_31_a-20211031.txt', 'A_1090872-A_AGILENT_TECHNOLOGIES_INC_2022_10_31_a-20221031.txt']\n"
     ]
    }
   ],
   "source": [
    "is_10k = True # TODO 配置当前代码运行什么txt的llm程序\n",
    "            # True运行10k \n",
    "            # False 运行 sustainable report\n",
    "\n",
    "if is_10k:\n",
    "    folder_path = '10k'  # 10k 部分\n",
    "else:\n",
    "    folder_path = 'PDF_Convert_Txts' # 年报部分\n",
    "\n",
    "txt_path_list = os.listdir(f\"{folder_path}\")\n",
    "txt_path_list = sorted(txt_path_list)\n",
    "\n",
    "\n",
    "\n",
    "if is_10k:\n",
    "    existed_report_path = f\"./Supply_10K_After_GPT/{COMPLETION_MODEL}\"\n",
    "else:\n",
    "    existed_report_path = f\"./Supply_sustainable_report_After_GPT/{COMPLETION_MODEL}\"\n",
    "\n",
    "\n",
    "print(f\"总共待处理文档{len(txt_path_list)}个\\n文档列表如下：\\n{txt_path_list= }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选取处理文档5个\n",
      " 文档列表如下：\n",
      "txt_path_list= ['AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2013_12_31_aagaa10k-20131231.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2014_12_31_d829913d10k.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2015_12_31_d78287d10k.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2016_12_31_d286458d10k.txt', 'AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2017_12_31_a10k123117.txt']\n"
     ]
    }
   ],
   "source": [
    "text_file_num = 5   # TODO 配置你想要测试的文档数量\n",
    "org_txt_path_list = txt_path_list.copy()\n",
    "txt_path_list = txt_path_list[:text_file_num] \n",
    "\n",
    "# 如果不想要配置文档数量并测试，而是开始全部爬取，则取下面的注释 TODO\n",
    "# txt_path_list = org_txt_path_list\n",
    "\n",
    "\n",
    "print(f\"选取处理文档{len(txt_path_list)}个\\n 文档列表如下：\\n{txt_path_list= }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "开始处理 company_name AAL_6201-AAL_AMERICAN_AIRLINES_GROUP_INC_2013_12_31_aagaa10k-20131231.txt\n",
      "len_extracted_text=45 Item 1.Business5Item1A.Risk Factors25Item 1B.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\junchengshen\\AppData\\Local\\Temp\\ipykernel_28884\\4224630594.py\", line 77, in <cell line: 28>\n",
      "    output = chat_gpt_turbo(message_our, COMPLETION_MODEL).content\n",
      "  File \"C:\\Users\\junchengshen\\AppData\\Local\\Temp\\ipykernel_28884\\1418857773.py\", line 21, in chat_gpt_turbo\n",
      "    completion = client.chat.completions.create(\n",
      "  File \"d:\\anaconda_0622\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 303, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\anaconda_0622\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 598, in create\n",
      "    return self._post(\n",
      "  File \"d:\\anaconda_0622\\lib\\site-packages\\openai\\_base_client.py\", line 1086, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"d:\\anaconda_0622\\lib\\site-packages\\openai\\_base_client.py\", line 846, in request\n",
      "    return self._request(\n",
      "  File \"d:\\anaconda_0622\\lib\\site-packages\\openai\\_base_client.py\", line 898, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-3Pv3O***************************************3POr. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no response from gpt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mchat_gpt_turbo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_our\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCOMPLETION_MODEL\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     78\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mchat_gpt_turbo\u001b[1;34m(message_our, COMPLETION_MODEL, n, max_tokens)\u001b[0m\n\u001b[0;32m     19\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mAPI_KEY)\n\u001b[1;32m---> 21\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCOMPLETION_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_our\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\anaconda_0622\\lib\\site-packages\\openai\\_utils\\_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda_0622\\lib\\site-packages\\openai\\resources\\chat\\completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda_0622\\lib\\site-packages\\openai\\_base_client.py:1086\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1083\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1084\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1085\u001b[0m )\n\u001b[1;32m-> 1086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\anaconda_0622\\lib\\site-packages\\openai\\_base_client.py:846\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    839\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    844\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    845\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda_0622\\lib\\site-packages\\openai\\_base_client.py:898\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    896\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 898\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-3Pv3O***************************************3POr. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno response from gpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attempts \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 断点重续\n",
    "try:\n",
    "    existed_reports = set()\n",
    "    existed_report_list = os.listdir(existed_report_path)\n",
    "    for existed_report in existed_report_list[:-1]:\n",
    "        # existed_report = existed_report.replace(\"_suppliers\", \"\")\n",
    "        existed_report = existed_report.split(\".\")[0]\n",
    "        existed_reports.add(existed_report)\n",
    "    print(existed_reports)\n",
    "\n",
    "    if len(existed_report_list) >= 1:\n",
    "        last_report = existed_report_list[-1] # 删除，重新生成，以保证完整性\n",
    "        print(f\"从文档{last_report}开始\")\n",
    "        import os\n",
    "        last_file_path = f\"{existed_report_path}/{last_report}\"# 替换为要删除的文件的路径\n",
    "        # print(last_file_path)\n",
    "        # 检查文件是否存在\n",
    "        if os.path.exists(last_file_path):\n",
    "            os.remove(last_file_path)\n",
    "            print(f\"File {last_file_path} has been deleted.\")\n",
    "        else:\n",
    "            print(f\"File {last_file_path} does not exist.\")\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "for txt_path in txt_path_list:\n",
    "    company_name = txt_path.replace(\".txt\", \"\")\n",
    "    print(\"开始处理 company_name\", txt_path)\n",
    "    if company_name in existed_reports:\n",
    "        continue\n",
    "    modified_text = read_and_replace_newlines(f\"{folder_path}/{txt_path}\")\n",
    "\n",
    "    extract_text = extract_10k_from_text(modified_text) # TODO 10k 特有步骤，非10k 可以删除\n",
    "    modified_text_parts = split_text_into_parts(extract_text)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        As an operations management researcher, \\\n",
    "        your task is to extract the names of suppliers from a company's provided responsibility report and summarize the overall situation of the supplier as mentioned in the report.\\\n",
    "        Strictly adhere to the following example format, without adding or altering any content\n",
    "        If it does not exist any supplier companies, an empty {} is returned.\\\n",
    "        If it does not exist any supplier companies, following format string is returned.    :\\\n",
    "        {\n",
    "            'Supplier Company Name': 'Summary of content about current Supplier',\n",
    "            ...\n",
    "        }.\\\n",
    "        Here is an output example:\\\n",
    "        {\n",
    "            'Umicore Finland Oy': 'Conformant refiner sourcing cobalt for Gigafactory Nevada and Fremont external cell sourcing, located in Finland.',\n",
    "            'Murrin Murrin Nickel Cobalt Plant': 'Conformant refiner sourcing cobalt for Gigafactory Nevada and Fremont external cell sourcing, located in Australia.',\n",
    "            ...\n",
    "        }.\\\n",
    "        Any deviation from this format will not be accepted. \\\n",
    "        Any information beyond this format will be disregarded. \\\n",
    "        Please ensure your response strictly follows this structure. \\\n",
    "        Extract the names of the suppliers accurately based on the report's content, and output them in the specified format.\n",
    "\n",
    "\"\"\"\n",
    "    to_save_data=dict()\n",
    "    for modified_text_item in modified_text_parts:\n",
    "\n",
    "        message_our = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ]\n",
    "\n",
    "\n",
    "        user_prompt = f\"{modified_text_item}\" \n",
    "        user_prompt_dic = {\"role\": \"user\", \"content\": user_prompt}\n",
    "        message_our.append(user_prompt_dic)\n",
    "\n",
    "        attempts = 0\n",
    "        success = False\n",
    "\n",
    "        while attempts < 5 and not success:\n",
    "            try:\n",
    "                output = chat_gpt_turbo(message_our, COMPLETION_MODEL).content\n",
    "                success = True\n",
    "                print(\"output\", output)\n",
    "            except Exception as ex:\n",
    "                traceback.print_exc()\n",
    "                print('no response from gpt')\n",
    "                time.sleep(5)\n",
    "                attempts += 1\n",
    "                if attempts == 3:\n",
    "                    break\n",
    "            \n",
    "\n",
    "        import re\n",
    "        if output and re.match(r'^\\s*\\{\\s*\\}\\s*$', output) is None:\n",
    "            data = extract_data(output, entity_extraction_func)\n",
    "\n",
    "        print(f\"from output extract {data=}\")\n",
    "        if data and isinstance(data, dict):\n",
    "            to_save_data.update(data)\n",
    "        else:\n",
    "            print(f\"data is not instance dict some error happend {data}\")\n",
    "\n",
    "    save_data(to_save_data, existed_report_path, company_name)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'company_name' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dylan\\Desktop\\code\\get_web_pdf_and_get_supplier_from_it\\src_code\\split_10k_merge_llm_prodict_and_process.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dylan/Desktop/code/get_web_pdf_and_get_supplier_from_it/src_code/split_10k_merge_llm_prodict_and_process.ipynb#W6sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m                 os\u001b[39m.\u001b[39mmakedirs(directory)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dylan/Desktop/code/get_web_pdf_and_get_supplier_from_it/src_code/split_10k_merge_llm_prodict_and_process.ipynb#W6sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m             result_df\u001b[39m.\u001b[39mto_csv(new_file_path, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dylan/Desktop/code/get_web_pdf_and_get_supplier_from_it/src_code/split_10k_merge_llm_prodict_and_process.ipynb#W6sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m process_all(existed_report_path)\n",
      "\u001b[1;32mc:\\Users\\dylan\\Desktop\\code\\get_web_pdf_and_get_supplier_from_it\\src_code\\split_10k_merge_llm_prodict_and_process.ipynb Cell 5\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dylan/Desktop/code/get_web_pdf_and_get_supplier_from_it/src_code/split_10k_merge_llm_prodict_and_process.ipynb#W6sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m result_df \u001b[39m=\u001b[39m process_supplier_data(df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dylan/Desktop/code/get_web_pdf_and_get_supplier_from_it/src_code/split_10k_merge_llm_prodict_and_process.ipynb#W6sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# 把company_name, year, type加到result_df后，每一行的值都相同\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dylan/Desktop/code/get_web_pdf_and_get_supplier_from_it/src_code/split_10k_merge_llm_prodict_and_process.ipynb#W6sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dylan/Desktop/code/get_web_pdf_and_get_supplier_from_it/src_code/split_10k_merge_llm_prodict_and_process.ipynb#W6sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m# 向结果DataFrame中添加公司名、年份和类型的列\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dylan/Desktop/code/get_web_pdf_and_get_supplier_from_it/src_code/split_10k_merge_llm_prodict_and_process.ipynb#W6sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m result_df[\u001b[39m'\u001b[39m\u001b[39mCompany\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m company_name\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dylan/Desktop/code/get_web_pdf_and_get_supplier_from_it/src_code/split_10k_merge_llm_prodict_and_process.ipynb#W6sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m result_df[\u001b[39m'\u001b[39m\u001b[39mYear\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m year\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dylan/Desktop/code/get_web_pdf_and_get_supplier_from_it/src_code/split_10k_merge_llm_prodict_and_process.ipynb#W6sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m result_df[\u001b[39m'\u001b[39m\u001b[39mReport_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m report_type\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'company_name' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# def standardize_supplier_name(name):\n",
    "#     \"\"\" 标准化供应商名称，可根据需要调整正则表达式 \"\"\"\n",
    "#     # 示例：将简写转换为完整名称，如“Tesla”转换为“Tesla Inc.”\n",
    "#     # 注意：这里的规则需要根据具体情况来定制\n",
    "#     name = re.sub(r'\\bTesla\\b', 'Tesla Inc.', name)\n",
    "#     return name\n",
    "\n",
    "def process_supplier_data(df):\n",
    "\n",
    "    # # 标准化供应商名称\n",
    "    # df['Supplier'] = df['Supplier'].apply(standardize_supplier_name)\n",
    "\n",
    "    # 确保所有的内容都是字符串类型\n",
    "    df['Content of Supplier'] = df['Content of Supplier'].astype(str)\n",
    "\n",
    "\n",
    "    # 合并相同的供应商行，拼接内容\n",
    "    df = df.groupby('Supplier')['Content of Supplier'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "    # 在结尾添加一行编号\n",
    "    df['Number'] = range(1, len(df) + 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def process_all(folder_path):\n",
    "\n",
    "    # 遍历文件夹中的所有文件\n",
    "    for filename in os.listdir(folder_path):\n",
    "        print(f\"{filename=}\")\n",
    "        if filename.endswith('.csv'):  # 确保文件是CSV格式\n",
    "\n",
    "            if not is_10k and filename.split(\".\")[0].endswith('CSR'):\n",
    "            \n",
    "                company_name = filename.split(\".\")[0].split(\"_\")[1]\n",
    "                year = filename.split(\".\")[0].split(\"_\")[2]\n",
    "                report_type = filename.split(\".\")[0].split(\"_\")[-1]\n",
    "                # print(company_name)\n",
    "                # print(year)\n",
    "                # print(report_type)\n",
    "            elif is_10k:\n",
    "                \n",
    "                # # 使用正则表达式匹配日期和年份部分\n",
    "                pattern = r\"(\\d{4})_(\\d{2}_\\d{2})\"\n",
    "                result = re.search(pattern, filename)\n",
    "\n",
    "                if result:\n",
    "                    year = result.group(1)\n",
    "                    date = result.group(2)\n",
    "                    print(\"Year:\", year)\n",
    "                    print(\"Date:\", date)\n",
    "                else:\n",
    "                    print(\"No match found.\")\n",
    "                result = re.search(pattern, filename)\n",
    "                tic = filename.split(\"-\")[0].split(\"_\")[0]\n",
    "                tic_company_name_pattern = r\"(\\w+)_(\\d{4}_\\d{2}_\\d{2})\"\n",
    "                tic_company_name_res = re.search(tic_company_name_pattern, filename)\n",
    "                if tic_company_name_res:\n",
    "                    company_name = result.group(1).strip(tic).strip(\"_\")\n",
    "                    print(\"TIC:\", tic)\n",
    "                    print(\"Company Name:\", company_name)\n",
    "                else:\n",
    "                    print(\"No match found.\")\n",
    "                company_name = company_name\n",
    "                report_type = \"10k\" if is_10k else \"sustainable report\"\n",
    "                # print(company_name)\n",
    "                # print(year)\n",
    "                # print(report_type)\n",
    "\n",
    "\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "            # 合并相同的\n",
    "            result_df = process_supplier_data(df)\n",
    "\n",
    "            # 把company_name, year, type加到result_df后，每一行的值都相同\n",
    "\n",
    "            # 向结果DataFrame中添加公司名、年份和类型的列\n",
    "            result_df['Company'] = company_name\n",
    "            result_df['Year'] = year\n",
    "            result_df['Report_type'] = report_type\n",
    "\n",
    "            # 需要代码补全\n",
    "\n",
    "            # 可以选择将结果保存回文件\n",
    "            new_folder_path = f\"{folder_path}_post\"\n",
    "            new_file_path = os.path.join(new_folder_path, f\"{filename}\")\n",
    "            # 检查路径是否存在，如果不存在，则创建它\n",
    "            directory = os.path.dirname(new_file_path)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            result_df.to_csv(new_file_path, index=False,sep=\"\\t\")\n",
    "\n",
    "\n",
    "process_all(\"./Supply_After_GPT/gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
